name: Daily International Basketball Scrape

on:
  schedule:
    # Run at 6:00 AM UTC every day
    - cron: '0 6 * * *'
  workflow_dispatch:
    # Allow manual trigger from GitHub UI

permissions:
  contents: write

jobs:
  scrape-euroleague:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests euroleague-api beautifulsoup4

      - name: Run EuroLeague scraper
        run: |
          cd scrapers/euroleague
          mkdir -p output/json
          python daily_scraper.py --recent
          python hometown_lookup_fixed.py || echo "Hometown lookup had issues, continuing..."
          python join_data.py

      - name: Copy EuroLeague data to output
        run: |
          mkdir -p output/json
          cp scrapers/euroleague/output/json/unified_american_players_latest.json output/json/euroleague_american_players_latest.json 2>/dev/null || true
          cp scrapers/euroleague/output/json/unified_american_players_latest.json output/json/euroleague_unified_players_latest.json 2>/dev/null || true

      - name: Upload EuroLeague artifacts
        uses: actions/upload-artifact@v4
        with:
          name: euroleague-data
          path: output/json/euroleague_*.json
          if-no-files-found: warn

  scrape-acb:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 flask

      - name: Run Liga ACB scrapers
        run: |
          cd scrapers/acb
          mkdir -p output/json
          python daily_scraper.py
          python acb_scraper.py
          python join_data.py

      - name: Copy ACB data to output
        run: |
          mkdir -p output/json
          cp scrapers/acb/output/json/unified_american_players_latest.json output/json/acb_american_players_latest.json 2>/dev/null || true
          cp scrapers/acb/output/json/unified_american_players_latest.json output/json/acb_unified_players_latest.json 2>/dev/null || true

      - name: Upload ACB artifacts
        uses: actions/upload-artifact@v4
        with:
          name: acb-data
          path: output/json/acb_*.json
          if-no-files-found: warn

  scrape-bsl:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 flask

      - name: Run Turkish BSL scrapers
        run: |
          cd scrapers/bsl
          mkdir -p output/json
          python daily_scraper.py
          python bsl_scraper.py
          python join_data.py

      - name: Copy BSL data to output
        run: |
          mkdir -p output/json
          cp scrapers/bsl/output/json/unified_american_players_latest.json output/json/bsl_american_players_latest.json 2>/dev/null || true
          cp scrapers/bsl/output/json/unified_american_players_latest.json output/json/bsl_unified_players_latest.json 2>/dev/null || true

      - name: Upload BSL artifacts
        uses: actions/upload-artifact@v4
        with:
          name: bsl-data
          path: output/json/bsl_*.json
          if-no-files-found: warn

  scrape-lnb:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4

      - name: Run French LNB scrapers
        run: |
          cd scrapers/lnb
          mkdir -p output/json
          python lnb_scraper.py
          python join_data.py

      - name: Copy LNB data to output
        run: |
          mkdir -p output/json
          cp scrapers/lnb/output/json/lnb_american_stats_latest.json output/json/lnb_american_players_latest.json 2>/dev/null || true
          cp scrapers/lnb/output/json/unified_american_players_latest.json output/json/lnb_unified_players_latest.json 2>/dev/null || true

      - name: Upload LNB artifacts
        uses: actions/upload-artifact@v4
        with:
          name: lnb-data
          path: output/json/lnb_*.json
          if-no-files-found: warn

  scrape-esake:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4

      - name: Run Greek ESAKE scrapers
        run: |
          cd scrapers/esake
          mkdir -p output/json
          python esake_scraper.py
          python esake_boxscore_scraper.py
          python join_data.py

      - name: Copy ESAKE data to output
        run: |
          mkdir -p output/json
          cp scrapers/esake/output/json/esake_american_stats_latest.json output/json/esake_american_players_latest.json 2>/dev/null || true
          cp scrapers/esake/output/json/unified_american_players_latest.json output/json/esake_unified_players_latest.json 2>/dev/null || true
          cp scrapers/esake/output/json/esake_boxscores_latest.json output/json/esake_boxscores_latest.json 2>/dev/null || true
          cp scrapers/esake/output/json/esake_games_latest.json output/json/esake_games_latest.json 2>/dev/null || true

      - name: Upload ESAKE artifacts
        uses: actions/upload-artifact@v4
        with:
          name: esake-data
          path: |
            output/json/esake_*.json
          if-no-files-found: warn

  combine-and-commit:
    runs-on: ubuntu-latest
    needs: [scrape-euroleague, scrape-acb, scrape-bsl, scrape-lnb, scrape-esake]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: output/json/
          merge-multiple: true

      - name: List combined data
        run: ls -la output/json/

      - name: Configure Git
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

      - name: Commit and push changes
        run: |
          git add output/json/*_latest.json
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            DATE=$(date +'%Y-%m-%d %H:%M UTC')
            git commit -m "Daily scrape: $DATE [automated]"
            git pull --rebase origin main || true
            git push
            echo "Changes committed and pushed"
          fi

      - name: Summary
        run: |
          echo "## Daily Scrape Complete" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date +'%Y-%m-%d %H:%M UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Data Files" >> $GITHUB_STEP_SUMMARY
          ls -la output/json/*.json 2>/dev/null | tail -20 >> $GITHUB_STEP_SUMMARY || echo "No JSON files found" >> $GITHUB_STEP_SUMMARY
